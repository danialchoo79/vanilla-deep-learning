{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a867b47-66ed-42b7-9390-c2f9977916c6",
   "metadata": {},
   "source": [
    "## **Building first Neural Network with TensorFlow and Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236dc83b-0d9f-445e-8582-01bcad883e56",
   "metadata": {},
   "source": [
    "#### **Things to know:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8287e19-f057-4764-8a99-3ab9b88af4e3",
   "metadata": {},
   "source": [
    "#### Definition of terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed438df8-714c-479b-a9e3-b261ed7ad644",
   "metadata": {},
   "source": [
    "1. Sequential(): A Sequential model is appropriate for a plain stack of layers where each layer has exactly one input tensor and one output tensor.\n",
    "\n",
    "2. Dense(): A dense layer is a layer that is deeply connected with its preceding layer which means the neurons of the layer are connected to every neuron of its preceding layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36967b83-f838-4749-9e71-12041ced5529",
   "metadata": {},
   "source": [
    "**Arguments:**\n",
    "\n",
    "**units**: Positive integer, dimensionality of the output space <br>\n",
    "**activation**: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x) <br>\n",
    "**use_bias**: Boolean, whether the layer uses a bias vector. <br>\n",
    "**kernel_initilizer**: Initializer for the kernel weights matrix. <br>\n",
    "**bias_initializer**: Initializer for bias vector <br>\n",
    "**bias_regularizer**: Regularizer function applied to the bias vector. <br>\n",
    "**activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\") <br>\n",
    "**kernel_constraint**: Constraint function applied to the kernel weights matrix <br>\n",
    "**bias_constraint**: Constraint function applied to the bias vector. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d0d4d-92a2-4b20-904b-4d996a42e49c",
   "metadata": {},
   "source": [
    "Compile(): putting all our recipes together and making it ready for training in the next phase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d566f-2e0f-4d88-8a24-a4b8e800ee33",
   "metadata": {},
   "source": [
    "*model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                       tf.keras.metrics.FalseNegatives()])*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee167e9e-a986-42f0-985d-8f8c11a7c4ec",
   "metadata": {},
   "source": [
    "fit(): Trains the model for a fixed number of epochs (iterations on a dataset). <br>\n",
    "\n",
    "1. **epoch**: One pass through all of the rows in the training dataset. <br>\n",
    "2. **batch**: one or more samples considered by the model within an epoch before weights are updated. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca9a577-8c50-41f2-a612-7c0649d6f358",
   "metadata": {},
   "source": [
    "## **What is a Sample?**\n",
    "A sample is a single row of data. <br>\n",
    "\n",
    "It contains inputs that are fed into the algorithm and an output that is used to compare to the prediction and calculate an error. <br>\n",
    "\n",
    "A training dataset is comprised of many rows of data, e.g. many samples. A sample may also be called an instance, an observation, an input vector, or a feature vector. <br>\n",
    "\n",
    "Now that we know what a sample is, let's define a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41492e5e-5b97-4bb8-a071-96c11475306f",
   "metadata": {},
   "source": [
    "## **What is a Batch?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7518a74-cac1-4d25-8fae-43d99a1cd773",
   "metadata": {},
   "source": [
    "The batch is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. <br>\n",
    "\n",
    "Thingk of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient. <br>\n",
    "\n",
    "A training dataset can be divided into one or more batches. <br>\n",
    "\n",
    "When all training samples are used to create one batch, the learning algorithm is called batch gradient descent. When the batch is the size of one sample, the learning algorithm is called stochastic gradient descent. When the batch size is more than one sample and less than the size of the training data set, the learning algorithm is called mini-batch gradient descent <br>\n",
    "\n",
    "1. **Batch Gradient Descent**: Batch Size = Size of Training Set <br>\n",
    "2. **Stochastic Gradient Descent**: Batch Size = 1 <br>\n",
    "3. **Mini-Batch Gradent Descent**: Batch Size < Size of Training Set <br>\n",
    "\n",
    "In the case of mini-bath gradient descent, popular batch sizes include 32,64, and 128 samples. You may see these values used in models in literature and in tutorials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4804da1c-b3d7-4626-9b6b-7326f89a0989",
   "metadata": {},
   "source": [
    "## **What is an Epoch?**\n",
    "The number of epochs is a hyperparameter that defines the number of times that he learning algorithm will work through the entire training dataset. <br>\n",
    "\n",
    "One epoch means that each sample in the training dataset has had an opportunity to update the internal model parameters. An epoch is comprised of one or more batches. For example, as above, an epoch that has one batch is called the batch gradient descent learning algorithm. <br>\n",
    "\n",
    "You can think of a for-loop over the number of epochs where each loop proceeds over the training dataset. Within ths for-loop is another nested for-loop that iterates over every batch of samples, where one batch has the specified \"batch size\" number of samples. <br>\n",
    "\n",
    "The number of epochs is traditionally large, often hundreds or thousands, allowing the learning algorithm to run until the error from the model has been sufficiently minimized. You may see examples of the number of epochs in the literature and in tutorials set to 10, 100, 500, 1000 and larger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7fabf-0761-4dc2-b52e-bdc83d8e3ee5",
   "metadata": {},
   "source": [
    "## **What is the Difference between Batch and Epoch**\n",
    "1) The batch size is a number of samples processed before the model is updated <br>\n",
    "2) The number of eopochs is the number of complete passes through the training dataset <br>\n",
    "3) The size of a batch must be more than or equal to one or less than or equal to the number of samples in the training dataset. <br>\n",
    "4) The number of epochs can be set to an integer value between one and infinity. You can run the algorithm for as long as you like and even stop it using other criteria besides a fixed number of epochs, such as change (or lack of change) in model error over time.\n",
    "5) They are both integer in values and are both hyperparametersfor the learning algorithm, e.g. parameters for th learning process, not internal model parameters found by the learning process.\n",
    "6) You must specify the batch size and number of epochs for a learning algorithm.\n",
    "7) There are no magic rules for how to configure these parameters. You must try different values and see what works best for your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20669391-008d-4c1d-ae56-d83e76924f71",
   "metadata": {},
   "source": [
    "## **Worked Example**\n",
    "\n",
    "Finally, let's make this concrete with a small example. <br>\n",
    "\n",
    "Assume you have a dataset with 200 samples (rows of data) and you choose a batch size of 5 and 1,000 epoch <br>\n",
    "\n",
    "This means that the dataset will be divided into 40 batches, each with five samples. The model wights will be updated after each batch of five samples. <br>\n",
    "\n",
    "This also means that one epoch will involve 40 batches or 40 updates to the model. <br>\n",
    "\n",
    "With 1000 epochs, the model will be exposed to or pass through the whole dataset 1,000 times. That is a total of 40,000 batches during the entire training process. <br>\n",
    "\n",
    "Note the folloring steps in creating your neural network: <br>\n",
    "1. Define our training data <br>\n",
    "2. Create a sequential model: <br>\n",
    "3. Add layers <br>\n",
    "4. Compile our model <br>\n",
    "5. Predict values <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c291f-edb2-481d-8ee3-eb94b1e93807",
   "metadata": {},
   "source": [
    "## **Building A Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd013e-4f2a-46df-b4d8-beb161bc68cb",
   "metadata": {},
   "source": [
    "## **1. Load the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26f2db-cb27-4c60-9918-5895cc031b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow\n",
    "\n",
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c52ceb80-6c68-42c7-b2ab-2be7c2991b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7be06ed-f9dc-4bb6-9932-436e7606028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = loadtxt(\"pima-indians-diabetes.data.csv\", delimiter = ',')\n",
    "X = dataset [:,0:8]\n",
    "y= dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a03c93ef-3897-4398-9b9c-5811ab66a402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "       0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
       "       0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       0., 1., 0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb7be7-39e2-49d9-a2b4-25984d895abd",
   "metadata": {},
   "source": [
    "**Note:** the dataset has 9 columns and the range 0:8 will select columns from 0 to 7, stopping before index 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d08cd85-4adb-4967-a4ad-476223c16249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  6.   , 148.   ,  72.   , ...,  33.6  ,   0.627,  50.   ],\n",
       "       [  1.   ,  85.   ,  66.   , ...,  26.6  ,   0.351,  31.   ],\n",
       "       [  8.   , 183.   ,  64.   , ...,  23.3  ,   0.672,  32.   ],\n",
       "       ...,\n",
       "       [  5.   , 121.   ,  72.   , ...,  26.2  ,   0.245,  30.   ],\n",
       "       [  1.   , 126.   ,  60.   , ...,  30.1  ,   0.349,  47.   ],\n",
       "       [  1.   ,  93.   ,  70.   , ...,  30.4  ,   0.315,  23.   ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "921dd728-cb3b-4576-8d50-4eebe509c052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>10.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>32.9</td>\n",
       "      <td>0.171</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>2.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.8</td>\n",
       "      <td>0.340</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>5.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>26.2</td>\n",
       "      <td>0.245</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.349</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.315</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0      1     2     3      4     5      6     7\n",
       "0     6.0  148.0  72.0  35.0    0.0  33.6  0.627  50.0\n",
       "1     1.0   85.0  66.0  29.0    0.0  26.6  0.351  31.0\n",
       "2     8.0  183.0  64.0   0.0    0.0  23.3  0.672  32.0\n",
       "3     1.0   89.0  66.0  23.0   94.0  28.1  0.167  21.0\n",
       "4     0.0  137.0  40.0  35.0  168.0  43.1  2.288  33.0\n",
       "..    ...    ...   ...   ...    ...   ...    ...   ...\n",
       "763  10.0  101.0  76.0  48.0  180.0  32.9  0.171  63.0\n",
       "764   2.0  122.0  70.0  27.0    0.0  36.8  0.340  27.0\n",
       "765   5.0  121.0  72.0  23.0  112.0  26.2  0.245  30.0\n",
       "766   1.0  126.0  60.0   0.0    0.0  30.1  0.349  47.0\n",
       "767   1.0   93.0  70.0  31.0    0.0  30.4  0.315  23.0\n",
       "\n",
       "[768 rows x 8 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "X_df = pd.DataFrame(X)\n",
    "X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e4072-c282-45ed-bc3e-ee130b055de2",
   "metadata": {},
   "source": [
    "## **2.Define the Keras Model**\n",
    "The first thing to get right is to ensure the input layer has the right number of input features. This can be specified when creating the first layer with the **input_dm** argument and setting it to 8 for the 8 input variables. <br>\n",
    "\n",
    "How do we know the number of layers and their types? <br>\n",
    "\n",
    "This is a very hard question. There are heuristics that we can use and often the best network structure is found through a process of trial and error experimentation. Generally, you need a network large enough to capture the structure of the problem. <br>\n",
    "\n",
    "In this example, we will use a fully-connected network structure with three layers. <br>\n",
    "\n",
    "Fully connected layers are defined using the Dense class. We can specify the number of neurons or nodes in the layer as the first argument, and specify the activation function using the activation argument. <br>\n",
    "\n",
    "We will use the rectified linear unit activation function related to as ReLU on the first two layers and the Sigmoid function in the output layer. <br>\n",
    "\n",
    "It used to be the case that Sigmoid function and Tanh activation functions were preferrred for all layers. These days, better performance is achieved using the ReLU activation function. We use a sigmoid on the outer layer to ensure out network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5 <br>\n",
    "\n",
    "We can piece it all together by adding each layer: <br>\n",
    "1. The model expects rows of data with 8 variables (the input_dim=8 argument) <br>\n",
    "2. The first hidden layer has 12 nodes and uses the relu activation function. <br>\n",
    "3. The second hidden layer has 8 nodes and uses the relu activation function. <br>\n",
    "4. The output layer has one node and uses the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f837f39-5a7a-4c02-a1eb-1e0d82b82c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=12, input_dim=8, activation='relu')) #1st hidden layer\n",
    "model.add(Dense(units=9, activation='relu')) #2nd hidden layer\n",
    "model.add(Dense(units=1,activation='sigmoid')) #3rd hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144684f7-5dbd-4b6c-8298-604b5c43cb53",
   "metadata": {},
   "source": [
    "**Note**: the most confusin thing here is that the shape of the input to the model is defined as the first argument on the first hidden layer. This means that the line of code that adds the first Dense layer is doing 2 things, defining the input or visible layer and the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214acceb-8dee-4b59-8c1c-d6562a4514c4",
   "metadata": {},
   "source": [
    "## **3. Compile keras Model**\n",
    "Now that the model is defined, we can compile it.\n",
    "\n",
    "We must specify the loss function to use to evaluate a set of weights, the opimizer is used to search through different weights for the network and any optional metrics we would like to collect and report during training. <br>\n",
    "\n",
    "This case, we will use cross entropy as the loss argument. This loss is for a binary classification problem and is defined in Keras as \"binary_crossentropy\". <br>\n",
    "\n",
    "We will define the optimizer as the efficient stochastic gradient algorithm \"adam\". This is a popular version of gradient descent because it automatically turns itself and gives good results in a wide range of problems. <br>\n",
    "\n",
    "Finally, because it is a classification problem, we will collect and report the classfication accuracy, defined via the metrics argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "172f51cd-476f-4697-a01d-fe3c0073bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64388672-a40e-42e8-b867-29b68140a699",
   "metadata": {},
   "source": [
    "## **4. Fit Keras Model**\n",
    "\n",
    "We have defined our model and compiled it ready for efficient training. <br>\n",
    "\n",
    "Now it is time to execute the model on some data <br>\n",
    "\n",
    "Training occurs over **epochs** and each epoch is split into **batches**. <br>\n",
    "\n",
    "1. **Epochs**: One pass through all of the row in the training data set. <br>\n",
    "2. **Batches**: One or more samples considered by the model within an epoch before weights are updated. <br>\n",
    "\n",
    "The training process will run for a fixed numberof iterations through the dataset called epochs, that we must specify using the epochs argument. We must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size and set using the batch_size argument. <br>\n",
    "\n",
    "For this problem, we will run for a small number of epochs (150) an use a relatively small batch size of 10. <br>\n",
    "\n",
    "These configurations can be chosen experimentally by trial  and error. We want to train the model enough o it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the emount of error will leve out after some point for a given model configuration. This is called model convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7a37238-370a-4c6e-8057-aadb0f08def6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "77/77 [==============================] - 1s 1ms/step - loss: 7.6550 - accuracy: 0.4570\n",
      "Epoch 2/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.8052 - accuracy: 0.5651\n",
      "Epoch 3/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 1.4214 - accuracy: 0.5859\n",
      "Epoch 4/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 1.2607 - accuracy: 0.6068\n",
      "Epoch 5/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 1.1662 - accuracy: 0.6016\n",
      "Epoch 6/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 1.1013 - accuracy: 0.6276\n",
      "Epoch 7/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 1.0691 - accuracy: 0.5990\n",
      "Epoch 8/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.9574 - accuracy: 0.6224\n",
      "Epoch 9/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.9353 - accuracy: 0.6055\n",
      "Epoch 10/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.9231 - accuracy: 0.6198\n",
      "Epoch 11/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.9079 - accuracy: 0.6393\n",
      "Epoch 12/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8973 - accuracy: 0.6133\n",
      "Epoch 13/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8891 - accuracy: 0.6367\n",
      "Epoch 14/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8122 - accuracy: 0.6341\n",
      "Epoch 15/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8223 - accuracy: 0.6419\n",
      "Epoch 16/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8137 - accuracy: 0.6445\n",
      "Epoch 17/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.8488 - accuracy: 0.6484\n",
      "Epoch 18/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.7694 - accuracy: 0.6471\n",
      "Epoch 19/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7702 - accuracy: 0.6484\n",
      "Epoch 20/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7523 - accuracy: 0.6263\n",
      "Epoch 21/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7292 - accuracy: 0.6536\n",
      "Epoch 22/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7566 - accuracy: 0.6367\n",
      "Epoch 23/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7214 - accuracy: 0.6680\n",
      "Epoch 24/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.7121 - accuracy: 0.6693\n",
      "Epoch 25/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6885 - accuracy: 0.6693\n",
      "Epoch 26/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6978 - accuracy: 0.6823\n",
      "Epoch 27/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6592 - accuracy: 0.6888\n",
      "Epoch 28/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6456 - accuracy: 0.6784\n",
      "Epoch 29/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6360 - accuracy: 0.6862\n",
      "Epoch 30/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6454 - accuracy: 0.6875\n",
      "Epoch 31/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6257 - accuracy: 0.6758\n",
      "Epoch 32/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6125 - accuracy: 0.6823\n",
      "Epoch 33/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6324 - accuracy: 0.6966\n",
      "Epoch 34/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6145 - accuracy: 0.6992\n",
      "Epoch 35/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6215 - accuracy: 0.6810\n",
      "Epoch 36/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6029 - accuracy: 0.7057\n",
      "Epoch 37/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.6901\n",
      "Epoch 38/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6169 - accuracy: 0.6940\n",
      "Epoch 39/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5981 - accuracy: 0.7083\n",
      "Epoch 40/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6009 - accuracy: 0.7018\n",
      "Epoch 41/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5944 - accuracy: 0.7057\n",
      "Epoch 42/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6185 - accuracy: 0.6901\n",
      "Epoch 43/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5993 - accuracy: 0.6927\n",
      "Epoch 44/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5953 - accuracy: 0.6979\n",
      "Epoch 45/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5863 - accuracy: 0.7122\n",
      "Epoch 46/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5834 - accuracy: 0.7135\n",
      "Epoch 47/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5770 - accuracy: 0.7122\n",
      "Epoch 48/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5768 - accuracy: 0.7135\n",
      "Epoch 49/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5826 - accuracy: 0.7096\n",
      "Epoch 50/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5771 - accuracy: 0.7227\n",
      "Epoch 51/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7214\n",
      "Epoch 52/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5679 - accuracy: 0.7148\n",
      "Epoch 53/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5917 - accuracy: 0.7148\n",
      "Epoch 54/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6013 - accuracy: 0.6914\n",
      "Epoch 55/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5853 - accuracy: 0.7201\n",
      "Epoch 56/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.7083\n",
      "Epoch 57/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.7227\n",
      "Epoch 58/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5709 - accuracy: 0.7174\n",
      "Epoch 59/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5493 - accuracy: 0.7279\n",
      "Epoch 60/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5739 - accuracy: 0.7057\n",
      "Epoch 61/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5678 - accuracy: 0.7227\n",
      "Epoch 62/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7305\n",
      "Epoch 63/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.7227\n",
      "Epoch 64/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5666 - accuracy: 0.7122\n",
      "Epoch 65/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5514 - accuracy: 0.7174\n",
      "Epoch 66/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7292\n",
      "Epoch 67/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5528 - accuracy: 0.7044\n",
      "Epoch 68/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5492 - accuracy: 0.7344\n",
      "Epoch 69/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5433 - accuracy: 0.7305\n",
      "Epoch 70/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.7083\n",
      "Epoch 71/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7591\n",
      "Epoch 72/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7266\n",
      "Epoch 73/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5599 - accuracy: 0.7435\n",
      "Epoch 74/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5576 - accuracy: 0.7188\n",
      "Epoch 75/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5455 - accuracy: 0.7214\n",
      "Epoch 76/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5346 - accuracy: 0.7357\n",
      "Epoch 77/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7448\n",
      "Epoch 78/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5381 - accuracy: 0.7357\n",
      "Epoch 79/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5391 - accuracy: 0.7422\n",
      "Epoch 80/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5390 - accuracy: 0.7357\n",
      "Epoch 81/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5352 - accuracy: 0.7305\n",
      "Epoch 82/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5353 - accuracy: 0.7201\n",
      "Epoch 83/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5458 - accuracy: 0.7279\n",
      "Epoch 84/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5373 - accuracy: 0.7227\n",
      "Epoch 85/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5424 - accuracy: 0.7214\n",
      "Epoch 86/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.7253\n",
      "Epoch 87/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5372 - accuracy: 0.7487\n",
      "Epoch 88/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5439 - accuracy: 0.7227\n",
      "Epoch 89/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5334 - accuracy: 0.7370\n",
      "Epoch 90/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5228 - accuracy: 0.7526\n",
      "Epoch 91/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5183 - accuracy: 0.7591\n",
      "Epoch 92/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5403 - accuracy: 0.7174\n",
      "Epoch 93/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5367 - accuracy: 0.7344\n",
      "Epoch 94/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5138 - accuracy: 0.7500\n",
      "Epoch 95/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7500\n",
      "Epoch 96/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5196 - accuracy: 0.7292\n",
      "Epoch 97/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5563 - accuracy: 0.7370\n",
      "Epoch 98/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5086 - accuracy: 0.7500\n",
      "Epoch 99/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5135 - accuracy: 0.7448\n",
      "Epoch 100/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5194 - accuracy: 0.7474\n",
      "Epoch 101/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5338 - accuracy: 0.7292\n",
      "Epoch 102/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5201 - accuracy: 0.7344\n",
      "Epoch 103/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5227 - accuracy: 0.7448\n",
      "Epoch 104/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5159 - accuracy: 0.7487\n",
      "Epoch 105/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5063 - accuracy: 0.7409\n",
      "Epoch 106/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5135 - accuracy: 0.7578\n",
      "Epoch 107/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5269 - accuracy: 0.7161\n",
      "Epoch 108/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7487\n",
      "Epoch 109/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7630\n",
      "Epoch 110/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5092 - accuracy: 0.7513\n",
      "Epoch 111/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5043 - accuracy: 0.7630\n",
      "Epoch 112/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5303 - accuracy: 0.7357\n",
      "Epoch 113/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5227 - accuracy: 0.7448\n",
      "Epoch 114/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4963 - accuracy: 0.7539\n",
      "Epoch 115/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5324 - accuracy: 0.7188\n",
      "Epoch 116/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5179 - accuracy: 0.7370\n",
      "Epoch 117/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4929 - accuracy: 0.7578\n",
      "Epoch 118/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7396\n",
      "Epoch 119/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5298 - accuracy: 0.7448\n",
      "Epoch 120/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5205 - accuracy: 0.7318\n",
      "Epoch 121/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5160 - accuracy: 0.7357\n",
      "Epoch 122/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7461\n",
      "Epoch 123/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5126 - accuracy: 0.7435\n",
      "Epoch 124/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5130 - accuracy: 0.7526\n",
      "Epoch 125/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5374 - accuracy: 0.7370\n",
      "Epoch 126/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5426 - accuracy: 0.7201\n",
      "Epoch 127/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4996 - accuracy: 0.7604\n",
      "Epoch 128/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.7643\n",
      "Epoch 129/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7721\n",
      "Epoch 130/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5152 - accuracy: 0.7344\n",
      "Epoch 131/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.7578\n",
      "Epoch 132/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7435\n",
      "Epoch 133/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.7383\n",
      "Epoch 134/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.7799\n",
      "Epoch 135/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4873 - accuracy: 0.7656\n",
      "Epoch 136/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7552\n",
      "Epoch 137/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7526\n",
      "Epoch 138/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5158 - accuracy: 0.7565\n",
      "Epoch 139/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7396\n",
      "Epoch 140/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7409\n",
      "Epoch 141/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4976 - accuracy: 0.7565\n",
      "Epoch 142/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4917 - accuracy: 0.7591\n",
      "Epoch 143/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4987 - accuracy: 0.7487\n",
      "Epoch 144/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5081 - accuracy: 0.7383\n",
      "Epoch 145/150\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4947 - accuracy: 0.7448\n",
      "Epoch 146/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7474\n",
      "Epoch 147/150\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4971 - accuracy: 0.7526\n",
      "Epoch 148/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7656\n",
      "Epoch 149/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7591\n",
      "Epoch 150/150\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5032 - accuracy: 0.7513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24da3859ea0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,epochs=150,batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba0249-ca0c-4d8a-9910-f6cb5fe9b1c1",
   "metadata": {},
   "source": [
    "We can see that we have **768** records, so out of that, we choose **10** records at a time(referred to as the batch size) for **1** iteration (referred to as one Epoch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec09a09-8ae7-4289-821c-0b497301c30d",
   "metadata": {},
   "source": [
    "## **5.Evaluate Keras Model**\n",
    "\n",
    "We have trained our neural networks on the entire training dataset and we can evaluate the performance of the network on the same dataset. <br>\n",
    "\n",
    "You an evaluate your model on your training dataset using the evaluate() function on your model and pass it the same input and output used to train the model. <br>\n",
    "\n",
    "This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy, <br>\n",
    "\n",
    "The evaluate() function will return a list with two values. The first will be the lost of the model on the dataset and second will be the accuracy of the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "254313ef-b238-422c-bef3-546702ef4725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5615 - accuracy: 0.7174\n",
      "Accuracy: 71.74479365348816\n",
      "Loss: 0.5615082383155823\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X,y)\n",
    "print('Accuracy: {}\\nLoss: {}'.format(accuracy*100,loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3910d8a3-5aaa-4ea4-a586-5457b54ef160",
   "metadata": {},
   "source": [
    "## **7. Make predictions**\n",
    "\n",
    "Making predictions is easy as calling the predict() function. We are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1. We can easily convert them into a crisp binary prediction for this classification task by rounding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32950182-bd00-4220-871d-710f1571036e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 650us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X)\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83aac65e-289e-42f0-b1a8-44253f873169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 650us/step\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0] => 0 (expected 1)\n",
      "[1.0, 85.0, 66.0, 29.0, 0.0, 26.6, 0.351, 31.0] => 0 (expected 0)\n",
      "[8.0, 183.0, 64.0, 0.0, 0.0, 23.3, 0.672, 32.0] => 1 (expected 1)\n",
      "[1.0, 89.0, 66.0, 23.0, 94.0, 28.1, 0.167, 21.0] => 0 (expected 0)\n",
      "[0.0, 137.0, 40.0, 35.0, 168.0, 43.1, 2.288, 33.0] => 1 (expected 1)\n",
      "[5.0, 116.0, 74.0, 0.0, 0.0, 25.6, 0.201, 30.0] => 0 (expected 0)\n",
      "[3.0, 78.0, 50.0, 32.0, 88.0, 31.0, 0.248, 26.0] => 0 (expected 1)\n",
      "[10.0, 115.0, 0.0, 0.0, 0.0, 35.3, 0.134, 29.0] => 0 (expected 0)\n",
      "[2.0, 197.0, 70.0, 45.0, 543.0, 30.5, 0.158, 53.0] => 0 (expected 1)\n",
      "[8.0, 125.0, 96.0, 0.0, 0.0, 0.0, 0.232, 54.0] => 0 (expected 1)\n",
      "[4.0, 110.0, 92.0, 0.0, 0.0, 37.6, 0.191, 30.0] => 0 (expected 0)\n",
      "[10.0, 168.0, 74.0, 0.0, 0.0, 38.0, 0.537, 34.0] => 1 (expected 1)\n",
      "[10.0, 139.0, 80.0, 0.0, 0.0, 27.1, 1.441, 57.0] => 0 (expected 0)\n",
      "[1.0, 189.0, 60.0, 23.0, 846.0, 30.1, 0.398, 59.0] => 1 (expected 1)\n",
      "[5.0, 166.0, 72.0, 19.0, 175.0, 25.8, 0.587, 51.0] => 1 (expected 1)\n",
      "[7.0, 100.0, 0.0, 0.0, 0.0, 30.0, 0.484, 32.0] => 0 (expected 1)\n",
      "[0.0, 118.0, 84.0, 47.0, 230.0, 45.8, 0.551, 31.0] => 0 (expected 1)\n",
      "[7.0, 107.0, 74.0, 0.0, 0.0, 29.6, 0.254, 31.0] => 0 (expected 1)\n",
      "[1.0, 103.0, 30.0, 38.0, 83.0, 43.3, 0.183, 33.0] => 0 (expected 0)\n",
      "[1.0, 115.0, 70.0, 30.0, 96.0, 34.6, 0.529, 32.0] => 0 (expected 1)\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(X) > 0.5).astype(int)\n",
    "#summarize the first 20 cases\n",
    "for i in range(20):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i],y[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
